{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article  \n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://in.mashable.com/\"\n",
    "r=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(r.content,'html5lib') \n",
    "table=soup.findAll('a',attrs={'class':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=[]\n",
    "for row in table: \n",
    "    if not row['href'].startswith('http'):\n",
    "        news.append('https://in.mashable.com'+row['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in news:\n",
    "    article = Article(i, language=\"en\")\n",
    "    article.download() \n",
    "    article.parse() \n",
    "    article.nlp() \n",
    "    data={}\n",
    "    data['Title']=article.title\n",
    "    data['Text']=article.text\n",
    "    data['Summary']=article.summary\n",
    "    data['Keywords']=article.keywords\n",
    "    df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ftc, tool, facebooks, agreement, peoples, app...</td>\n",
       "      <td>Facebook announced last week that a federal co...</td>\n",
       "      <td>Facebook announced last week that a federal co...</td>\n",
       "      <td>U.S. Court Approves Facebook’s $5 Billion Priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[learning, improve, service, develop, stanford...</td>\n",
       "      <td>This streaming service used the Fugu algorithm...</td>\n",
       "      <td>At a time when millions are stuck at home and ...</td>\n",
       "      <td>Stanford Researchers Develop Algorithm To Impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[charger, india, support, pro, leak, suggests,...</td>\n",
       "      <td>OnePlus recently launched the OnePlus 8 and On...</td>\n",
       "      <td>OnePlus recently launched the OnePlus 8 and On...</td>\n",
       "      <td>OnePlus Warp Charge 30 Wireless Charger Leak S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[north, wonsan, health, jongun, rumours, train...</td>\n",
       "      <td>Over the past week, North Korean leader Kim Jo...</td>\n",
       "      <td>Over the past week, North Korean leader Kim Jo...</td>\n",
       "      <td>Is Kim Jong-un Alive? Here’s What We Know Abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[director, movie, marvel, sam, scenes, action,...</td>\n",
       "      <td>Firstly because it was Chris Hemsworth's first...</td>\n",
       "      <td>The buildup to Extraction was quite thrilling....</td>\n",
       "      <td>Exclusive: 'Extraction' Director Sam Hargrave ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Keywords  \\\n",
       "0  [ftc, tool, facebooks, agreement, peoples, app...   \n",
       "1  [learning, improve, service, develop, stanford...   \n",
       "2  [charger, india, support, pro, leak, suggests,...   \n",
       "3  [north, wonsan, health, jongun, rumours, train...   \n",
       "4  [director, movie, marvel, sam, scenes, action,...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Facebook announced last week that a federal co...   \n",
       "1  This streaming service used the Fugu algorithm...   \n",
       "2  OnePlus recently launched the OnePlus 8 and On...   \n",
       "3  Over the past week, North Korean leader Kim Jo...   \n",
       "4  Firstly because it was Chris Hemsworth's first...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Facebook announced last week that a federal co...   \n",
       "1  At a time when millions are stuck at home and ...   \n",
       "2  OnePlus recently launched the OnePlus 8 and On...   \n",
       "3  Over the past week, North Korean leader Kim Jo...   \n",
       "4  The buildup to Extraction was quite thrilling....   \n",
       "\n",
       "                                               Title  \n",
       "0  U.S. Court Approves Facebook’s $5 Billion Priv...  \n",
       "1  Stanford Researchers Develop Algorithm To Impr...  \n",
       "2  OnePlus Warp Charge 30 Wireless Charger Leak S...  \n",
       "3  Is Kim Jong-un Alive? Here’s What We Know Abou...  \n",
       "4  Exclusive: 'Extraction' Director Sam Hargrave ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.DataFrame(df)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH=\"OnlineNewsPopularity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=pd.read_csv(FILEPATH)\n",
    "data1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cols={x: x.lower().strip() for x in list(data1)}\n",
    "data1=data1.rename(index=str,columns=clean_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data1, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train = train_set.drop(['url','shares', 'timedelta','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess'], axis=1)\n",
    "y_train = train_set['shares']\n",
    "\n",
    "x_test = test_set.drop(['url','shares', 'timedelta','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess'], axis=1)\n",
    "y_test = test_set['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=RandomForestRegressor(random_state=42)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(clf.predict(x_train),list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual shares</th>\n",
       "      <th>Predicted shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900</td>\n",
       "      <td>2016.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>1777.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>790</td>\n",
       "      <td>922.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>872</td>\n",
       "      <td>1273.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2500</td>\n",
       "      <td>2629.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual shares  Predicted shares\n",
       "0           1900           2016.82\n",
       "1           1800           1777.14\n",
       "2            790            922.30\n",
       "3            872           1273.54\n",
       "4           2500           2629.21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reset_index(level=0, inplace=True)\n",
    "result_df = result.rename(index=str, columns={\"index\": \"Actual shares\", 0: \"Predicted shares\"})\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186018\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISHAL\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['call', 'infrastructur', 'protect', 'summit']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] ==2].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(67259 unique tokens: ['broadcast', 'commun', 'decid', 'licenc', 'awar']...)\n",
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n",
      "11 aust\n",
      "12 rise\n",
      "13 staff\n",
      "14 strike\n",
      "15 affect\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print(dictionary)\n",
    "count=0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1), (8, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 (\"call\") appears 1 time.\n",
      "Word 8 (\"infrastructur\") appears 1 time.\n",
      "Word 9 (\"protect\") appears 1 time.\n",
      "Word 10 (\"summit\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_2 = bow_corpus[2]\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                               dictionary[bow_doc_2[i][0]], \n",
    "bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"polic\" + 0.016*\"charg\" + 0.014*\"court\" + 0.012*\"death\" + 0.012*\"murder\" + 0.011*\"crash\" + 0.009*\"woman\" + 0.008*\"face\" + 0.008*\"kill\" + 0.007*\"attack\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"australia\" + 0.013*\"year\" + 0.010*\"australian\" + 0.008*\"open\" + 0.006*\"win\" + 0.006*\"coast\" + 0.006*\"record\" + 0.006*\"gold\" + 0.006*\"north\" + 0.006*\"queensland\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"trump\" + 0.010*\"interview\" + 0.009*\"fund\" + 0.009*\"rural\" + 0.009*\"news\" + 0.008*\"farmer\" + 0.008*\"say\" + 0.008*\"health\" + 0.007*\"chang\" + 0.007*\"warn\"\n",
      "Topic: 3 \n",
      "Words: 0.010*\"hous\" + 0.009*\"council\" + 0.008*\"plan\" + 0.008*\"elect\" + 0.008*\"countri\" + 0.005*\"hour\" + 0.005*\"power\" + 0.005*\"govern\" + 0.005*\"commun\" + 0.004*\"head\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"world\" + 0.008*\"sydney\" + 0.008*\"water\" + 0.007*\"time\" + 0.006*\"say\" + 0.006*\"green\" + 0.005*\"island\" + 0.005*\"royal\" + 0.005*\"govern\" + 0.005*\"elect\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4493105411529541\t \n",
      "Topic: 0.012*\"trump\" + 0.010*\"interview\" + 0.009*\"fund\" + 0.009*\"rural\" + 0.009*\"news\"\n",
      "\n",
      "Score: 0.42877814173698425\t \n",
      "Topic: 0.012*\"world\" + 0.008*\"sydney\" + 0.008*\"water\" + 0.007*\"time\" + 0.006*\"say\"\n",
      "\n",
      "Score: 0.041346970945596695\t \n",
      "Topic: 0.010*\"hous\" + 0.009*\"council\" + 0.008*\"plan\" + 0.008*\"elect\" + 0.008*\"countri\"\n",
      "\n",
      "Score: 0.04031626507639885\t \n",
      "Topic: 0.022*\"australia\" + 0.013*\"year\" + 0.010*\"australian\" + 0.008*\"open\" + 0.006*\"win\"\n",
      "\n",
      "Score: 0.040248073637485504\t \n",
      "Topic: 0.026*\"polic\" + 0.016*\"charg\" + 0.014*\"court\" + 0.012*\"death\" + 0.012*\"murder\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LDA(text):\n",
    "    unseen_document = text\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "    result=[]\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        result.append(score)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_unique_tokens_rate(words):\n",
    "    words=tokenize(words)\n",
    "    no_order = list(set(words))\n",
    "    unique_tokens_rate=len(no_order)/len(words)\n",
    "    return unique_tokens_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_nonstop_words_rate(words):\n",
    "    words=tokenize(words)\n",
    "    filtered_sentence = [w for w in words if not w in stopwords]\n",
    "    nonstop_words_rate=len(filtered_sentence)/len(words)\n",
    "    return nonstop_words_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_unique_nonstop_words_rate(words):\n",
    "    words=tokenize(words)\n",
    "    filtered_sentence = [w for w in words if not w in stopwords]\n",
    "    no_order = list(set(filtered_sentence))\n",
    "    unique_nonstop_words_rate=len(no_order)/len(words)\n",
    "    return unique_nonstop_words_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_token_length(words):\n",
    "    words=tokenize(words)\n",
    "    length=[]\n",
    "    for i in words:\n",
    "        length.append(len(i))\n",
    "    return np.average(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import datefinder\n",
    "import datetime  \n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day(article_text):\n",
    "    article=article_text\n",
    "    if len(list(datefinder.find_dates(article_text)))>0:\n",
    "        date=str(list(datefinder.find_dates(article_text))[0])\n",
    "        date=date.split()\n",
    "        date=date[0]\n",
    "        year, month, day = date.split('-')     \n",
    "        day_name = datetime.date(int(year), int(month), int(day)) \n",
    "        return day_name.strftime(\"%A\")\n",
    "    return \"Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text=text\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_neutral_tokens(text):\n",
    "    processed_docs = preprocess(text)\n",
    "    speech=nltk.pos_tag(processed_docs)\n",
    "    tags=[\"NN\",\"NNP\",\"NNS\",\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"UH\"]\n",
    "    speech_list=[i[0] for i in speech if i[1] in tags]\n",
    "    return len(speech_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words=[]\n",
    "negative_words=[]\n",
    "def polar(words):\n",
    "    all_tokens=tokenize(words)\n",
    "    for i in all_tokens:\n",
    "        analysis=TextBlob(i)\n",
    "        polarity=analysis.sentiment.polarity\n",
    "        if polarity>0:\n",
    "            positive_words.append(i)\n",
    "        if polarity<0:\n",
    "            negative_words.append(i)\n",
    "    return positive_words,negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(words):\n",
    "    words=polar(words)\n",
    "    positive=words[0]\n",
    "    negative=words[1]\n",
    "    global_rate_positive_words=(len(positive)/len(words))/100\n",
    "    global_rate_negative_words=(len(negative)/len(words))/100\n",
    "    polar_positive=[]\n",
    "    polar_negative=[]\n",
    "    for i in positive:\n",
    "        analysis=TextBlob(i)\n",
    "        polar_positive.append(analysis.sentiment.polarity)\n",
    "        avg_positive_polarity=analysis.sentiment.polarity\n",
    "    for j in negative:\n",
    "        analysis2=TextBlob(j)\n",
    "        polar_negative.append(analysis2.sentiment.polarity)\n",
    "        avg_negative_polarity=analysis2.sentiment.polarity\n",
    "    min_positive_polarity=min(polar_positive)\n",
    "    max_positive_polarity=max(polar_positive)\n",
    "    min_negative_polarity=min(polar_negative)\n",
    "    max_negative_polarity=max(polar_negative)\n",
    "    avg_positive_polarity=np.average(polar_positive)\n",
    "    avg_negative_polarity=np.average(polar_negative)\n",
    "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=[]\n",
    "for i in news:\n",
    "    pred_info={}\n",
    "    article = Article(i, language=\"en\") # en for English \n",
    "    article.download() \n",
    "    article.parse()\n",
    "    analysis=TextBlob(article.text)\n",
    "    polarity=analysis.sentiment.polarity\n",
    "    title_analysis=TextBlob(article.title)\n",
    "    pred_info['text']=article.text\n",
    "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
    "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
    "    pred_info['n_unique_tokens']=for_unique_tokens_rate(article.text)\n",
    "    pred_info['n_non_stop_words']=for_nonstop_words_rate(article.text)\n",
    "    pred_info['n_non_stop_unique_tokens']=for_unique_nonstop_words_rate(article.text)\n",
    "    pred_info['num_hrefs']=article.html.count(\"https://in.mashable.com\")\n",
    "    pred_info['num_imgs']=len(article.images)\n",
    "    pred_info['num_videos']=len(article.movies)\n",
    "    pred_info['average_token_length']=avg_token_length(article.text)\n",
    "    pred_info['num_keywords']=len(article.keywords)\n",
    "    \n",
    "    if \"life-style\" in article.url:\n",
    "        pred_info['data_channel_is_lifestyle']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_lifestyle']=0\n",
    "    if \"etimes\" in article.url:\n",
    "        pred_info['data_channel_is_entertainment']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_entertainment']=0\n",
    "    if \"business\" in article.url:\n",
    "        pred_info['data_channel_is_bus']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_bus']=0\n",
    "    if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
    "        data_channel_is_socmed=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_socmed=0\n",
    "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
    "        data_channel_is_tech=1\n",
    "        data_channel_is_socmed=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_tech=0\n",
    "    if \"world\" in article.url:\n",
    "        data_channel_is_world=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_socmed=0\n",
    "    else:\n",
    "        data_channel_is_world=0\n",
    "        \n",
    "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
    "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
    "    pred_info['data_channel_is_world']=data_channel_is_world\n",
    "    \n",
    "    if day(i)==\"Monday\":\n",
    "        pred_info['weekday_is_monday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_monday']=0\n",
    "    if day(i)==\"Tuesday\":\n",
    "        pred_info['weekday_is_tuesday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_tuesday']=0\n",
    "    if day(i)==\"Wednesday\":\n",
    "        pred_info['weekday_is_wednesday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_wednesday']=0\n",
    "    if day(i)==\"Thursday\":\n",
    "        pred_info['weekday_is_thursday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_thursday']=0\n",
    "    if day(i)==\"Friday\":\n",
    "        pred_info['weekday_is_friday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_friday']=0\n",
    "    if day(i)==\"Saturday\":\n",
    "        pred_info['weekday_is_saturday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_saturday']=0\n",
    "    if day(i)==\"Sunday\":\n",
    "        pred_info['weekday_is_sunday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_sunday']=0\n",
    "        pred_info['is_weekend']=0\n",
    "        \n",
    "    pred_info['LDA_00']=get_LDA(article.text)[0]\n",
    "    pred_info['LDA_01']=get_LDA(article.text)[1]\n",
    "    pred_info['LDA_02']=get_LDA(article.text)[2]\n",
    "    pred_info['LDA_03']=get_LDA(article.text)[3]\n",
    "    pred_info['LDA_04']=get_LDA(article.text)[4]\n",
    "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
    "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
    "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
    "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
    "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
    "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
    "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
    "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
    "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
    "    pred_info['max_negative_polarity']=rates(article.text)[7]\n",
    "    pred_info['rate_positive_words']=len(polar(article.text)[0])/non_neutral_tokens(article.text)\n",
    "    pred_info['rate_negative_words']=len(polar(article.text)[1])/non_neutral_tokens(article.text)\n",
    "    pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
    "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
    "    pred_info['abs_title_subjectivity']=abs(title_analysis.sentiment.subjectivity-0.5)\n",
    "    pred_info['abs_title_sentiment_polarity']=abs(title_analysis.sentiment.polarity)\n",
    "    df1.append(pred_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>...</th>\n",
       "      <th>text</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.359953</td>\n",
       "      <td>0.334193</td>\n",
       "      <td>0.112306</td>\n",
       "      <td>0.109795</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.584656</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.309861</td>\n",
       "      <td>...</td>\n",
       "      <td>Facebook announced last week that a federal co...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.252359</td>\n",
       "      <td>0.217821</td>\n",
       "      <td>0.186469</td>\n",
       "      <td>0.177869</td>\n",
       "      <td>0.165486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.652695</td>\n",
       "      <td>-0.198889</td>\n",
       "      <td>0.289338</td>\n",
       "      <td>...</td>\n",
       "      <td>At a time when millions are stuck at home and ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.270169</td>\n",
       "      <td>0.233210</td>\n",
       "      <td>0.199938</td>\n",
       "      <td>0.193831</td>\n",
       "      <td>0.102847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.283976</td>\n",
       "      <td>-0.229518</td>\n",
       "      <td>0.276925</td>\n",
       "      <td>...</td>\n",
       "      <td>OnePlus recently launched the OnePlus 8 and On...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309274</td>\n",
       "      <td>0.258175</td>\n",
       "      <td>0.228486</td>\n",
       "      <td>0.111496</td>\n",
       "      <td>0.092567</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>4.370874</td>\n",
       "      <td>-0.227222</td>\n",
       "      <td>0.279629</td>\n",
       "      <td>...</td>\n",
       "      <td>Over the past week, North Korean leader Kim Jo...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380239</td>\n",
       "      <td>0.225929</td>\n",
       "      <td>0.146265</td>\n",
       "      <td>0.137576</td>\n",
       "      <td>0.109989</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>4.396396</td>\n",
       "      <td>-0.227804</td>\n",
       "      <td>0.277646</td>\n",
       "      <td>...</td>\n",
       "      <td>The buildup to Extraction was quite thrilling....</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0  0.359953  0.334193  0.112306  0.109795  0.083752   \n",
       "1  0.252359  0.217821  0.186469  0.177869  0.165486   \n",
       "2  0.270169  0.233210  0.199938  0.193831  0.102847   \n",
       "3  0.309274  0.258175  0.228486  0.111496  0.092567   \n",
       "4  0.380239  0.225929  0.146265  0.137576  0.109989   \n",
       "\n",
       "   abs_title_sentiment_polarity  abs_title_subjectivity  average_token_length  \\\n",
       "0                      0.000000                0.500000              4.584656   \n",
       "1                      0.000000                0.500000              4.652695   \n",
       "2                      0.000000                0.500000              4.283976   \n",
       "3                      0.100000                0.100000              4.370874   \n",
       "4                      0.027778                0.305556              4.396396   \n",
       "\n",
       "   avg_negative_polarity  avg_positive_polarity  ...  \\\n",
       "0              -0.133333               0.309861  ...   \n",
       "1              -0.198889               0.289338  ...   \n",
       "2              -0.229518               0.276925  ...   \n",
       "3              -0.227222               0.279629  ...   \n",
       "4              -0.227804               0.277646  ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Facebook announced last week that a federal co...   \n",
       "1  At a time when millions are stuck at home and ...   \n",
       "2  OnePlus recently launched the OnePlus 8 and On...   \n",
       "3  Over the past week, North Korean leader Kim Jo...   \n",
       "4  The buildup to Extraction was quite thrilling....   \n",
       "\n",
       "   title_sentiment_polarity  title_subjectivity  weekday_is_friday  \\\n",
       "0                  0.000000            0.000000                  0   \n",
       "1                  0.000000            0.000000                  0   \n",
       "2                  0.000000            0.000000                  0   \n",
       "3                  0.100000            0.400000                  0   \n",
       "4                 -0.027778            0.194444                  0   \n",
       "\n",
       "   weekday_is_monday  weekday_is_saturday  weekday_is_sunday  \\\n",
       "0                  0                    0                  1   \n",
       "1                  1                    0                  0   \n",
       "2                  0                    0                  0   \n",
       "3                  1                    0                  0   \n",
       "4                  1                    0                  0   \n",
       "\n",
       "   weekday_is_thursday  weekday_is_tuesday  weekday_is_wednesday  \n",
       "0                    0                   0                     0  \n",
       "1                    0                   0                     0  \n",
       "2                    1                   0                     0  \n",
       "3                    0                   0                     0  \n",
       "4                    0                   0                     0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df=pd.DataFrame(df1)\n",
    "pred_test=pred_df.drop(['text'],axis=1)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Virality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook announced last week that a federal co...</td>\n",
       "      <td>63566.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At a time when millions are stuck at home and ...</td>\n",
       "      <td>72694.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnePlus recently launched the OnePlus 8 and On...</td>\n",
       "      <td>66897.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Over the past week, North Korean leader Kim Jo...</td>\n",
       "      <td>75233.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The buildup to Extraction was quite thrilling....</td>\n",
       "      <td>75452.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here's an odd fact: Apple Music is now availab...</td>\n",
       "      <td>75111.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bill Gates has already pledged $250 million to...</td>\n",
       "      <td>72636.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A newly discovered bug will crash your Apple i...</td>\n",
       "      <td>75233.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maybe it's that we've all been in quarantine f...</td>\n",
       "      <td>74786.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Being away from the people we love is hard. Pa...</td>\n",
       "      <td>79128.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The top health authority in the U.S. have upda...</td>\n",
       "      <td>75965.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>If someone asks me what are some of my fondest...</td>\n",
       "      <td>78044.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BTS member Suga will soon be releasing a brand...</td>\n",
       "      <td>75305.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The world we live in is an immoral one, where ...</td>\n",
       "      <td>75452.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Apple's new flagship iPhones might come a litt...</td>\n",
       "      <td>69708.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sports are on the back burner right now (for o...</td>\n",
       "      <td>74241.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>After the PM Narendra Modi of India had announ...</td>\n",
       "      <td>68581.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>If I began this by saying, \"Can you imagine it...</td>\n",
       "      <td>78203.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  Virality\n",
       "0   Facebook announced last week that a federal co...  63566.25\n",
       "1   At a time when millions are stuck at home and ...  72694.64\n",
       "2   OnePlus recently launched the OnePlus 8 and On...  66897.04\n",
       "3   Over the past week, North Korean leader Kim Jo...  75233.65\n",
       "4   The buildup to Extraction was quite thrilling....  75452.64\n",
       "5   Here's an odd fact: Apple Music is now availab...  75111.04\n",
       "6   Bill Gates has already pledged $250 million to...  72636.24\n",
       "7   A newly discovered bug will crash your Apple i...  75233.65\n",
       "8   Maybe it's that we've all been in quarantine f...  74786.64\n",
       "9   Being away from the people we love is hard. Pa...  79128.51\n",
       "10  The top health authority in the U.S. have upda...  75965.98\n",
       "11  If someone asks me what are some of my fondest...  78044.12\n",
       "12  BTS member Suga will soon be releasing a brand...  75305.64\n",
       "13  The world we live in is an immoral one, where ...  75452.64\n",
       "14  Apple's new flagship iPhones might come a litt...  69708.94\n",
       "15  Sports are on the back burner right now (for o...  74241.78\n",
       "16  After the PM Narendra Modi of India had announ...  68581.40\n",
       "17  If I began this by saying, \"Can you imagine it...  78203.51"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2=pd.DataFrame(clf.predict(pred_test),pred_df['text'])\n",
    "test2.reset_index(level=0, inplace=True)\n",
    "test2 = test2.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
